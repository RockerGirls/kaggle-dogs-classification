{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T02:52:30.944726Z",
     "start_time": "2018-12-22T02:52:29.605963Z"
    }
   },
   "outputs": [],
   "source": [
    "#导入所需要的包和模块\n",
    "import collections#collections是Python内建的一个集合模块，提供了许多有用的集合类\n",
    "import gluonbook as gb\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, model_zoo, nn\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "data_dir = './zjc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T02:52:30.976180Z",
     "start_time": "2018-12-22T02:52:30.946893Z"
    }
   },
   "outputs": [],
   "source": [
    "#从整理数据集开始，reorg_train_valid函数从完整原始训练集中切分出验证集\n",
    "def reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label):\n",
    "    # 训练集中数量最少一类的狗的样本数。\n",
    "    min_n_train_per_label = (\n",
    "        collections.Counter(idx_label.values()).most_common()[:-2:-1][0][1])\n",
    "    # 验证集中每类狗的样本数。\n",
    "    n_valid_per_label = math.floor(min_n_train_per_label * valid_ratio)\n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = train_file.split('.')[0]\n",
    "        label = idx_label[idx]\n",
    "        gb.mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                    os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
    "            gb.mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            gb.mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'train', label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T02:52:31.214321Z",
     "start_time": "2018-12-22T02:52:30.978384Z"
    }
   },
   "outputs": [],
   "source": [
    "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio):\n",
    "    # 读取训练数据标签。\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        # 跳过文件头行（栏名称）。\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict(((idx, label) for idx, label in tokens))\n",
    "    reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label)\n",
    "    # 整理测试集。\n",
    "    gb.mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(data_dir, input_dir, 'test', 'unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T02:57:33.915233Z",
     "start_time": "2018-12-22T02:52:31.935579Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:#demo:\n",
    "    # 注意：此处使用小数据集并将批量大小相应设小。使用 Kaggle 比赛的完整数据集时可设批量大\n",
    "    # 小为较大整数。\n",
    "    input_dir, batch_size = '', 1\n",
    "else:\n",
    "    label_file, train_dir, test_dir = 'labels.csv', 'train', 'test'\n",
    "    input_dir, batch_size, valid_ratio = 'train_valid_test', 128, 0.1\n",
    "    reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T02:57:33.935568Z",
     "start_time": "2018-12-22T02:57:33.918830Z"
    }
   },
   "outputs": [],
   "source": [
    "#训练集的数据增强\n",
    "#对图像进行不同方式的裁剪、调整亮度、色彩等。\n",
    "transform_train = gdata.vision.transforms.Compose([\n",
    "    # 随机对图像裁剪出面积为原图像面积 0.08 到 1 倍之间、且高和宽之比在 3/4 和 4/3 之间\n",
    "    # 的图像，再放缩为高和宽均为 224 像素的新图像。\n",
    "\n",
    "    gdata.vision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n",
    "                                              ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    gdata.vision.transforms.RandomFlipLeftRight(),\n",
    "    # 随机变化亮度、对比度和饱和度。\n",
    "    gdata.vision.transforms.RandomColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                              saturation=0.4),\n",
    "    # 随机加噪音。\n",
    "    gdata.vision.transforms.RandomLighting(0.1),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    # 对图像的每个通道做标准化。\n",
    "    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练集的数据增强\n",
    "#图像增强：对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。\n",
    "#可以降低模型对某些属性的依赖，从而提高模型的泛化能力。\n",
    "#例如，对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而让模型减轻对物体出现位置的依赖性。还有调整亮度、色彩等。\n",
    "transform_train = gdata.vision.transforms.Compose([\n",
    "    # 随机对图像裁剪出面积为原图像面积 0.08 到 1 倍之间、且高和宽之比在 3/4 和 4/3 之间\n",
    "    # 的图像，再放缩为高和宽均为 224 像素的新图像。\n",
    "\n",
    "    gdata.vision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n",
    "                                              ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    gdata.vision.transforms.RandomFlipLeftRight(),\n",
    "    # 随机变化亮度、对比度和饱和度。\n",
    "    gdata.vision.transforms.RandomColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                              saturation=0.4),\n",
    "    # 随机加噪音。\n",
    "    gdata.vision.transforms.RandomLighting(0.1),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    # 对图像的每个通道做标准化。\n",
    "    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T02:57:34.159036Z",
     "start_time": "2018-12-22T02:57:33.938163Z"
    }
   },
   "outputs": [],
   "source": [
    "#测试集的数据增强\n",
    "transform_test = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.Resize(256),\n",
    "    # 将图像中央的高和宽均为 224 的正方形区域裁剪出来。\n",
    "    gdata.vision.transforms.CenterCrop(224),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T02:57:35.326567Z",
     "start_time": "2018-12-22T02:57:34.162516Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#创建ImageFolderDataset实例来读取整理后的含原始图像文件的数据集\n",
    "train_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'train'), flag=1)\n",
    "valid_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'valid'), flag=1)\n",
    "train_valid_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'train_valid'), flag=1)\n",
    "test_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'test'), flag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T02:57:35.335246Z",
     "start_time": "2018-12-22T02:57:35.328679Z"
    }
   },
   "outputs": [],
   "source": [
    "#创建DataLoader实例\n",
    "train_data = gdata.DataLoader(train_ds.transform_first(transform_train),\n",
    "                              batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = gdata.DataLoader(valid_ds.transform_first(transform_test),\n",
    "                              batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data = gdata.DataLoader(train_valid_ds.transform_first(\n",
    "    transform_train), batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = gdata.DataLoader(test_ds.transform_first(transform_test),\n",
    "                             batch_size, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T04:52:15.572283Z",
     "start_time": "2018-12-23T04:52:15.553536Z"
    }
   },
   "outputs": [],
   "source": [
    "#定义模型：利用Gluon 的预训练模型，在这里使用了预训练的 ResNet152_v2 模型\n",
    "def get_net(ctx):\n",
    "    finetune_net = model_zoo.vision.resnet50_v2(pretrained=True)  #这里可以选择网络\n",
    "    # 定义新的输出网络。\n",
    "    finetune_net.output_new = nn.HybridSequential(prefix='')\n",
    "    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n",
    "    # 120 是输出的类别数。\n",
    "    finetune_net.output_new.add(nn.Dense(120))\n",
    "    # 初始化输出网络。\n",
    "    finetune_net.output_new.initialize(init.Xavier(), ctx=ctx)\n",
    "    # 把模型参数分配到即将用于计算的 CPU 或 GPU 上。\n",
    "    finetune_net.collect_params().reset_ctx(ctx)\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T04:52:20.307761Z",
     "start_time": "2018-12-23T04:52:20.298717Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def get_loss(data, net, ctx):\n",
    "    l = 0.0\n",
    "    for X, y in data:\n",
    "        y = y.as_in_context(ctx)\n",
    "        output_features = net.features(X.as_in_context(ctx))\n",
    "        outputs = net.output_new(output_features)\n",
    "        l += loss(outputs, y).mean().asscalar()\n",
    "    return l / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T04:52:30.797494Z",
     "start_time": "2018-12-23T04:52:30.701834Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_period,\n",
    "          lr_decay):\n",
    "    # 只训练我们定义的小规模输出网络。\n",
    "    trainer = gluon.Trainer(net.output_new.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    #trainer = gluon.Trainer(net.output_new.collect_params(), 'adam',\n",
    "    #                        {'learning_rate': lr})\n",
    "    switch_delr = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l, start = 0.0, time.time()\n",
    "        if epoch > 0 and switch_delr == 0:#epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in train_data:\n",
    "            y = y.astype('float32').as_in_context(ctx)\n",
    "            output_features = net.features(X.as_in_context(ctx))\n",
    "            with autograd.record():\n",
    "                outputs = net.output_new(output_features)\n",
    "                l = loss(outputs, y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l += l.mean().asscalar()\n",
    "        time_s = \"time %.2f sec\" % (time.time() - start)\n",
    "        if valid_data is not None:\n",
    "            global min_valloss,best_epoch\n",
    "            valid_loss = get_loss(valid_data, net, ctx)\n",
    "            if valid_loss < min_valloss:\n",
    "                min_valloss = valid_loss\n",
    "                best_epoch = epoch + 1\n",
    "                net.save_parameters('resnet50-best.parameters')\n",
    "                switch_delr = 5\n",
    "            else:\n",
    "                switch_delr -= 1\n",
    "            epoch_s = (\"epoch %d, train loss %f, valid loss %f, \"\n",
    "                       % (epoch + 1, train_l / len(train_data), valid_loss))\n",
    "        else:\n",
    "            epoch_s = (\"epoch %d, train loss %f, \"\n",
    "                       % (epoch + 1, train_l / len(train_data)))\n",
    "        print(epoch_s + time_s + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-23T04:52:34.051Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 3.421688, valid loss 1.402047, time 79.20 sec, lr 0.01\n",
      "epoch 2, train loss 1.264278, valid loss 0.661270, time 76.72 sec, lr 0.01\n",
      "epoch 3, train loss 0.944871, valid loss 0.518590, time 79.47 sec, lr 0.01\n",
      "epoch 4, train loss 0.853375, valid loss 0.445683, time 78.89 sec, lr 0.01\n",
      "epoch 5, train loss 0.816127, valid loss 0.437845, time 78.80 sec, lr 0.01\n",
      "epoch 6, train loss 0.775019, valid loss 0.402784, time 78.71 sec, lr 0.01\n",
      "epoch 7, train loss 0.761728, valid loss 0.402506, time 79.80 sec, lr 0.01\n",
      "epoch 8, train loss 0.730868, valid loss 0.401441, time 78.30 sec, lr 0.01\n",
      "epoch 9, train loss 0.723021, valid loss 0.392653, time 79.15 sec, lr 0.01\n",
      "epoch 10, train loss 0.697139, valid loss 0.376695, time 78.75 sec, lr 0.01\n",
      "epoch 11, train loss 0.709589, valid loss 0.394510, time 78.29 sec, lr 0.01\n",
      "epoch 12, train loss 0.667467, valid loss 0.352477, time 78.66 sec, lr 0.01\n",
      "epoch 13, train loss 0.667456, valid loss 0.369477, time 78.61 sec, lr 0.01\n",
      "epoch 14, train loss 0.664692, valid loss 0.350076, time 79.69 sec, lr 0.01\n",
      "epoch 15, train loss 0.667866, valid loss 0.356835, time 78.19 sec, lr 0.01\n",
      "epoch 16, train loss 0.640305, valid loss 0.351868, time 79.18 sec, lr 0.01\n",
      "epoch 17, train loss 0.650417, valid loss 0.347173, time 79.41 sec, lr 0.01\n",
      "epoch 18, train loss 0.634976, valid loss 0.344668, time 76.64 sec, lr 0.01\n",
      "epoch 19, train loss 0.634252, valid loss 0.345226, time 133.51 sec, lr 0.01\n",
      "epoch 20, train loss 0.643097, valid loss 0.355940, time 188.50 sec, lr 0.01\n",
      "epoch 21, train loss 0.631031, valid loss 0.354990, time 193.76 sec, lr 0.01\n",
      "epoch 22, train loss 0.623327, valid loss 0.335081, time 196.59 sec, lr 0.01\n",
      "epoch 23, train loss 0.616101, valid loss 0.352258, time 88.79 sec, lr 0.01\n",
      "epoch 24, train loss 0.600633, valid loss 0.352799, time 78.30 sec, lr 0.01\n",
      "epoch 25, train loss 0.599188, valid loss 0.348009, time 78.84 sec, lr 0.01\n",
      "epoch 26, train loss 0.615676, valid loss 0.354270, time 78.67 sec, lr 0.01\n",
      "epoch 27, train loss 0.612774, valid loss 0.363711, time 82.85 sec, lr 0.01\n",
      "epoch 28, train loss 0.615819, valid loss 0.333606, time 79.06 sec, lr 0.001\n",
      "epoch 29, train loss 0.554151, valid loss 0.319103, time 78.60 sec, lr 0.001\n",
      "epoch 30, train loss 0.536901, valid loss 0.320419, time 78.70 sec, lr 0.001\n",
      "epoch 31, train loss 0.536689, valid loss 0.311788, time 78.74 sec, lr 0.001\n",
      "epoch 32, train loss 0.534938, valid loss 0.316802, time 79.25 sec, lr 0.001\n",
      "epoch 33, train loss 0.543367, valid loss 0.312919, time 78.43 sec, lr 0.001\n",
      "epoch 34, train loss 0.538405, valid loss 0.316464, time 79.10 sec, lr 0.001\n",
      "epoch 35, train loss 0.569401, valid loss 0.318439, time 78.67 sec, lr 0.001\n",
      "epoch 36, train loss 0.567038, valid loss 0.315737, time 78.98 sec, lr 0.001\n",
      "epoch 37, train loss 0.549697, valid loss 0.314299, time 78.62 sec, lr 0.0001\n",
      "epoch 38, train loss 0.548822, valid loss 0.309997, time 78.91 sec, lr 0.0001\n",
      "epoch 39, train loss 0.556585, valid loss 0.306528, time 80.55 sec, lr 0.0001\n",
      "epoch 40, train loss 0.528906, valid loss 0.313134, time 77.64 sec, lr 0.0001\n",
      "epoch 41, train loss 0.546997, valid loss 0.312600, time 79.36 sec, lr 0.0001\n",
      "epoch 42, train loss 0.562513, valid loss 0.313873, time 79.21 sec, lr 0.0001\n",
      "epoch 43, train loss 0.541517, valid loss 0.311843, time 78.46 sec, lr 0.0001\n",
      "epoch 44, train loss 0.536700, valid loss 0.308255, time 78.02 sec, lr 0.0001\n",
      "epoch 45, train loss 0.532966, valid loss 0.324927, time 80.89 sec, lr 1e-05\n",
      "epoch 46, train loss 0.545417, valid loss 0.311414, time 79.62 sec, lr 1e-05\n",
      "epoch 47, train loss 0.546215, valid loss 0.310498, time 78.51 sec, lr 1e-05\n",
      "epoch 48, train loss 0.534286, valid loss 0.320645, time 78.93 sec, lr 1e-05\n",
      "epoch 49, train loss 0.565297, valid loss 0.316480, time 78.84 sec, lr 1e-05\n",
      "epoch 50, train loss 0.558002, valid loss 0.310337, time 79.26 sec, lr 1e-05\n",
      "epoch 51, train loss 0.523691, valid loss 0.319094, time 78.67 sec, lr 1e-05\n",
      "epoch 52, train loss 0.554294, valid loss 0.317302, time 79.56 sec, lr 1e-05\n",
      "epoch 53, train loss 0.544628, valid loss 0.322214, time 79.49 sec, lr 1e-05\n",
      "epoch 54, train loss 0.558048, valid loss 0.314873, time 78.81 sec, lr 1e-05\n",
      "epoch 55, train loss 0.578281, valid loss 0.311605, time 78.05 sec, lr 1e-05\n",
      "epoch 56, train loss 0.538187, valid loss 0.316218, time 78.94 sec, lr 1e-05\n"
     ]
    }
   ],
   "source": [
    "#训练并验证模型\n",
    "import mxnet as mx\n",
    "min_valloss = 9999\n",
    "best_epoch = 0\n",
    "ctx, num_epochs, lr, wd = mx.gpu(7), 100, 0.01, 1e-4 #gpu使用7\n",
    "lr_period, lr_decay, net = 8, 0.1, get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T11:04:52.355144Z",
     "start_time": "2018-12-23T11:04:52.347749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3052832509080569 93\n"
     ]
    }
   ],
   "source": [
    "print(min_valloss,best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T11:06:21.272964Z",
     "start_time": "2018-12-23T11:05:23.317307Z"
    }
   },
   "outputs": [],
   "source": [
    "#对测试集分类\n",
    "#net = get_net(ctx)\n",
    "#net.hybridize()\n",
    "#train(net, train_valid_data, None, num_epochs, lr, wd, ctx, lr_period,\n",
    "#      lr_decay)\n",
    "net.load_parameters('resnet50-best.parameters')\n",
    "preds = []\n",
    "for data, label in test_data:\n",
    "    output_features = net.features(data.as_in_context(ctx))\n",
    "    output = nd.softmax(net.output_new(output_features))\n",
    "    preds.extend(output.asnumpy())\n",
    "ids = sorted(os.listdir(os.path.join(data_dir, input_dir, 'test/unknown')))\n",
    "with open('submission-resnet50-loss0.30-threshold.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T16:11:19.964595Z",
     "start_time": "2018-12-23T16:11:19.858650Z"
    }
   },
   "outputs": [],
   "source": [
    "#继承学习\n",
    "#定义模型：利用Gluon 的预训练模型，在这里使用了预训练的 ResNet152_v2 模型\n",
    "def get_net1(ctx):\n",
    "    finetune_net = model_zoo.vision.resnet50_v2(pretrained=True)  #这里可以选择网络\n",
    "    # 定义新的输出网络。\n",
    "    finetune_net.output_new = nn.HybridSequential(prefix='')\n",
    "    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n",
    "    # 120 是输出的类别数。\n",
    "    finetune_net.output_new.add(nn.Dense(120))\n",
    "    # 初始化输出网络。\n",
    "    finetune_net.output_new.initialize(init.Xavier(), ctx=ctx)\n",
    "    # 把模型参数分配到即将用于计算的 CPU 或 GPU 上。\n",
    "    finetune_net.collect_params().reset_ctx(ctx)\n",
    "    return finetune_net\n",
    "#定义模型：利用Gluon 的预训练模型，在这里使用了预训练的 ResNet152_v2 模型\n",
    "def get_net2(ctx):\n",
    "    finetune_net = model_zoo.vision.resnet101_v2(pretrained=True)  #这里可以选择网络\n",
    "    # 定义新的输出网络。\n",
    "    finetune_net.output_new = nn.HybridSequential(prefix='')\n",
    "    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n",
    "    # 120 是输出的类别数。\n",
    "    finetune_net.output_new.add(nn.Dense(120))\n",
    "    # 初始化输出网络。\n",
    "    finetune_net.output_new.initialize(init.Xavier(), ctx=ctx)\n",
    "    # 把模型参数分配到即将用于计算的 CPU 或 GPU 上。\n",
    "    finetune_net.collect_params().reset_ctx(ctx)\n",
    "    return finetune_net\n",
    "#定义模型：利用Gluon 的预训练模型，在这里使用了预训练的 ResNet152_v2 模型\n",
    "def get_net3(ctx):\n",
    "    finetune_net = model_zoo.vision.resnet152_v2(pretrained=True)  #这里可以选择网络\n",
    "    # 定义新的输出网络。\n",
    "    finetune_net.output_new = nn.HybridSequential(prefix='')\n",
    "    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n",
    "    # 120 是输出的类别数。\n",
    "    finetune_net.output_new.add(nn.Dense(120))\n",
    "    # 初始化输出网络。\n",
    "    finetune_net.output_new.initialize(init.Xavier(), ctx=ctx)\n",
    "    # 把模型参数分配到即将用于计算的 CPU 或 GPU 上。\n",
    "    finetune_net.collect_params().reset_ctx(ctx)\n",
    "    return finetune_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T16:52:50.799448Z",
     "start_time": "2018-12-23T16:49:25.189342Z"
    }
   },
   "outputs": [],
   "source": [
    "net1 = get_net1(ctx)\n",
    "net2 = get_net2(ctx)\n",
    "net3 = get_net3(ctx)\n",
    "net1.load_parameters('resnet50-best.parameters')\n",
    "net2.load_parameters('resnet101-best.parameters')\n",
    "net3.load_parameters('resnet152-best-0.17783.parameters')\n",
    "preds1 = []\n",
    "preds2 = []\n",
    "preds3 = []\n",
    "for data, label in test_data:\n",
    "    output_features = net1.features(data.as_in_context(ctx))\n",
    "    output = nd.softmax(net1.output_new(output_features))\n",
    "    preds1.extend(output.asnumpy())\n",
    "for data, label in test_data:\n",
    "    output_features = net2.features(data.as_in_context(ctx))\n",
    "    output = nd.softmax(net2.output_new(output_features))\n",
    "    preds2.extend(output.asnumpy())\n",
    "for data, label in test_data:\n",
    "    output_features = net3.features(data.as_in_context(ctx))\n",
    "    output = nd.softmax(net3.output_new(output_features))\n",
    "    preds3.extend(output.asnumpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T17:12:14.682002Z",
     "start_time": "2018-12-23T17:12:13.506804Z"
    }
   },
   "outputs": [],
   "source": [
    "ids = sorted(os.listdir(os.path.join(data_dir, input_dir, 'test/unknown')))\n",
    "preds = [(0.18447*j + 0.25398*k) / 2.0 for j,k in zip( preds2, preds3)]\n",
    "with open('submission-all3-2.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T17:10:21.196900Z",
     "start_time": "2018-12-23T17:10:21.184650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.43845, 0.51854, 0.58805)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.77252-0.33407, 0.77252-0.25398, 0.77252-0.18447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T17:11:49.281572Z",
     "start_time": "2018-12-23T17:11:49.274518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18447000000000002, 0.25398)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.43845-0.25398, 0.43845-0.18447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:44:15.954718Z",
     "start_time": "2018-12-24T02:44:15.308524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10357"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir('./zjc/test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
